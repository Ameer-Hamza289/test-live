{% extends 'base.html' %}
{% load static %}


{% block content %}
<!-- Sub banner start -->
<div class="sub-banner overview-bgi">
    <div class="container breadcrumb-area">
        <div class="breadcrumb-areas">
            <h1>AI Retailor</h1>
            <ul class="breadcrumbs">
                <li><a href="{% url 'home' %}">Home</a></li>
                <li class="active">AI Retailor</li>
            </ul>
        </div>
    </div>
</div>
<div class="container py-5">
    <div class="row justify-content-center">
        <div class="col-md-6 col-lg-5" style="margin-top: 100px;">
            {% if user.is_authenticated %}
            <div class="voice-call-box text-center p-4 shadow rounded bg-white">
                <!-- Logo (optional, uncomment if you want to show) -->
                {# <img src="{% static 'img/logo.png' %}" alt="Logo" class="mb-3" style="height: 48px;"> #}
                <!-- <h3 class="mb-3">Voice Assistant</h3> -->
                <div id="callStatus" class="mb-2 fw-bold text-success" style="font-size: 1.2rem;">Ready</div>
                <div id="callTimer" class="mb-3 text-muted" style="font-size: 1.1rem;">00:00</div>

                <!-- Microphone Button -->
                <button id="micButton" class="btn btn-light btn-lg rounded-circle shadow mb-3"
                    style="width: 80px; height: 80px;">
                    <i id="micIcon" class="fa fa-microphone text-primary" style="font-size: 2rem;"></i>
                </button>

                <!-- Processing Indicator -->
                <div id="processingIndicator" class="mb-3 d-none">
                    <div class="spinner-border text-primary" role="status"></div>
                    <div class="mt-2">Assistant is processing...</div>
                </div>

                <!-- Current Response -->
                <div id="currentResponse" class="mb-3 p-3 bg-light rounded" style="min-height: 60px;">
                    <span class="text-muted">Waiting for your question...</span>
                </div>

                <!-- Download Transcript Button -->
                <button id="downloadTranscript" class="btn btn-outline-primary w-100" disabled>
                    <i class="fa fa-download"></i> Download Transcript
                </button>

                <!-- Feedback Form (initially hidden) -->
                <div id="feedbackForm" class="mt-4 p-3 border rounded bg-light d-none">
                    <h5 class="mb-3 text-center">How was your call experience?</h5>
                    
                    <!-- Rating Section -->
                    <div class="mb-3">
                        <label class="form-label">Rating:</label>
                        <div class="rating-container text-center">
                            <div class="star-rating" id="starRating">
                                <span class="star" data-rating="1">⭐</span>
                                <span class="star" data-rating="2">⭐</span>
                                <span class="star" data-rating="3">⭐</span>
                                <span class="star" data-rating="4">⭐</span>
                                <span class="star" data-rating="5">⭐</span>
                            </div>
                            <div class="rating-text mt-1">
                                <small id="ratingText" class="text-muted">Please select a rating</small>
                            </div>
                        </div>
                    </div>

                    <!-- Comments Section -->
                    <div class="mb-3">
                        <label for="feedbackComments" class="form-label">General Comments:</label>
                        <textarea id="feedbackComments" class="form-control" rows="2" 
                                  placeholder="How was your overall experience?"></textarea>
                    </div>

                    <!-- Helpful Aspects -->
                    <div class="mb-3">
                        <label for="helpfulAspects" class="form-label">What was most helpful?</label>
                        <textarea id="helpfulAspects" class="form-control" rows="2" 
                                  placeholder="What did you find most useful about this call?"></textarea>
                    </div>

                    <!-- Improvement Suggestions -->
                    <div class="mb-3">
                        <label for="improvementSuggestions" class="form-label">Suggestions for improvement:</label>
                        <textarea id="improvementSuggestions" class="form-control" rows="2" 
                                  placeholder="What could we improve?"></textarea>
                    </div>

                    <!-- Submit Buttons -->
                    <div class="d-flex gap-2">
                        <button id="submitFeedback" class="btn btn-primary flex-fill" disabled>
                            Submit Feedback
                        </button>
                        <button id="skipFeedback" class="btn btn-outline-secondary flex-fill">
                            Skip
                        </button>
                    </div>

                    <!-- Feedback Status -->
                    <div id="feedbackStatus" class="mt-2 text-center d-none">
                        <small class="text-success">Thank you for your feedback!</small>
                    </div>
                </div>

                <!-- Microphone Access Message -->
                <div id="micAccessMsg" class="text-danger mt-3 d-none">
                    This feature requires microphone access to work.
                </div>
            </div>
            {% else %}
            <div class="text-center p-4 shadow rounded bg-white">
                <h3 class="mb-3">Authentication Required</h3>
                <p class="mb-4">Please log in to access the voice assistant.</p>
                <a href="{% url 'login' %}" class="btn btn-primary">Log In</a>
            </div>
            {% endif %}
        </div>
    </div>
</div>

<style>
    .voice-call-box {
        max-width: 400px;
        width: 100%;
    }

    /* Add styles for listening indicator */
    .listening-indicator {
        position: absolute;
        bottom: -40px;
        left: 50%;
        transform: translateX(-50%);
        text-align: center;
        width: 100px;
    }

    .listening-waves {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 20px;
    }

    .wave {
        width: 3px;
        height: 15px;
        margin: 0 2px;
        background-color: #007bff;
        animation: wave 1s infinite ease-in-out;
    }

    .wave:nth-child(2) {
        animation-delay: 0.2s;
    }

    .wave:nth-child(3) {
        animation-delay: 0.4s;
    }

    @keyframes wave {

        0%,
        100% {
            height: 5px;
        }

        50% {
            height: 15px;
        }
    }

    .fa-microphone.listening {
        color: #28a745 !important;
        animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
        0% {
            opacity: 0.7;
        }

        50% {
            opacity: 1;
        }

        100% {
            opacity: 0.7;
        }
    }

    /* Add styles for speaking animation */
    #currentResponse.speaking {
        animation: highlight 2s infinite;
    }

    @keyframes highlight {
        0% {
            background-color: #f8f9fa;
        }

        50% {
            background-color: #e9ecef;
        }

        100% {
            background-color: #f8f9fa;
        }
    }

    /* Make the mic button position relative for absolute positioning of the indicator */
    #micButton {
        position: relative;
    }

    /* Feedback Form Styles */
    .star-rating {
        font-size: 1.5rem;
        cursor: pointer;
    }

    .star {
        display: inline-block;
        margin: 0 2px;
        transition: opacity 0.2s ease;
        opacity: 0.3;
    }

    .star:hover,
    .star.active {
        opacity: 1;
    }

    .star.active {
        filter: brightness(1.2);
    }

    #feedbackForm {
        animation: fadeIn 0.3s ease-in;
    }

    @keyframes fadeIn {
        from {
            opacity: 0;
            transform: translateY(-10px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }

    #feedbackForm .form-control {
        border-radius: 8px;
        border: 1px solid #dee2e6;
        transition: border-color 0.2s ease;
    }

    #feedbackForm .form-control:focus {
        border-color: #007bff;
        box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);
    }

    .btn:disabled {
        opacity: 0.6;
        cursor: not-allowed;
    }
</style>

<script>
    let recognition = null;
    let isCallActive = false;
    let callStartTime;
    let callTimer;
    let conversationLog = [];
    let callSessionId;
    let isProcessing = false;
    let lastProcessedText = '';
    let processingTimeout = null;
    let recognitionState = 'inactive'; // Can be 'inactive', 'starting', 'active', 'stopping'
    let recognitionRestartPending = false;
    let lastAssistantMessage = '';
    let isSpeaking = false;
    let listenIndicator = null; // Visual indicator for listening state
    let processingStuckSince = null;
    let selectedRating = 0;
    let feedbackSubmitted = false;

    function safeStartRecognition() {
        // Only start if we're in the right state
        if (recognitionState === 'inactive' && isCallActive) {
            try {
                recognitionState = 'starting';
                console.log('Starting speech recognition...');

                // Show visual listening indicator
                updateListeningIndicator(true);

                recognition.start();

                // Safety timeout - if recognition doesn't start within 2 seconds, force reinitialize
                setTimeout(() => {
                    if (recognitionState === 'starting') {
                        console.error('Recognition failed to start after timeout');
                        recognitionState = 'inactive';
                        initializeSpeechRecognition();
                        safeStartRecognition();
                    }
                }, 2000);
            } catch (e) {
                console.error('Error starting recognition:', e);
                recognitionState = 'inactive';
                updateListeningIndicator(false);
                // Try to create a new recognition object
                setTimeout(() => {
                    initializeSpeechRecognition();
                    safeStartRecognition();
                }, 500);
            }
        } else {
            console.log('Not starting recognition. State:', recognitionState, 'Call active:', isCallActive);
        }
    }

    function safeStopRecognition() {
        if (recognitionState === 'active' || recognitionState === 'starting') {
            recognitionState = 'stopping';
            console.log('Stopping speech recognition...');
            updateListeningIndicator(false);
            try {
                recognition.abort();
            } catch (e) {
                console.log('Error stopping recognition:', e);
                recognitionState = 'inactive';
            }
        } else {
            console.log('Recognition not active, cannot stop. State:', recognitionState);
            recognitionState = 'inactive';
        }
    }

    function initializeSpeechRecognition() {
        // Clean up any existing instance
        if (recognition) {
            try {
                safeStopRecognition();
                recognition.onstart = null;
                recognition.onresult = null;
                recognition.onerror = null;
                recognition.onend = null;
            } catch (e) {
                console.log('Error cleaning up recognition:', e);
            }
        }

        // Create fresh instance
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.continuous = false; // Process one utterance at a time
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        recognitionState = 'inactive';

        recognition.onstart = () => {
            document.getElementById('micAccessMsg').classList.add('d-none');
            recognitionState = 'active';
            console.log('Speech recognition active');

            // Update call status to show we're listening
            const callStatusEl = document.getElementById('callStatus');
            if (callStatusEl && isCallActive) {
                callStatusEl.textContent = 'Listening...';
            }
        };

        let currentTranscript = '';

        recognition.onresult = (event) => {
            // Only process if we're actively listening
            if (recognitionState !== 'active' || !isCallActive) return;

            const results = event.results;
            const result = results[results.length - 1];
            const transcript = result[0].transcript.trim();

            console.log('Speech detected:', transcript, 'isFinal:', result.isFinal);
            currentTranscript = transcript;

            // Update call status to show we heard something
            const callStatusEl = document.getElementById('callStatus');
            if (callStatusEl) {
                callStatusEl.textContent = 'Hearing you...';
            }

            if (result.isFinal &&
                !isProcessing &&
                currentTranscript &&
                currentTranscript !== lastProcessedText) {

                if (processingTimeout) {
                    clearTimeout(processingTimeout);
                }

                processingTimeout = setTimeout(() => {
                    console.log('Processing final transcript:', currentTranscript);
                    lastProcessedText = currentTranscript;
                    processVoiceInput(currentTranscript);
                    currentTranscript = '';
                }, 300);
            }
        };

        recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);

            if (event.error === 'not-allowed' || event.error === 'denied') {
                document.getElementById('micAccessMsg').classList.remove('d-none');
                endCallUI();
            } else if (event.error === 'aborted') {
                console.log('Recognition was aborted intentionally');
                recognitionState = 'inactive';
            } else {
                console.warn('Recognition error:', event.error);
                recognitionState = 'inactive';

                if (isCallActive && !isProcessing) {
                    console.log('Will try to restart after error');
                    setTimeout(safeStartRecognition, 200); // Reduced from 500ms
                }
            }
        };

        recognition.onend = () => {
            console.log('Recognition ended, state:', recognitionState);

            // Only transition to inactive if we're not in the middle of stopping
            if (recognitionState !== 'stopping') {
                recognitionState = 'inactive';

                if (isCallActive && !isProcessing) {
                    console.log('Recognition ended naturally, restarting immediately');
                    safeStartRecognition(); // Immediate restart with no delay
                }
            } else {
                // We were stopping intentionally
                recognitionState = 'inactive';
                console.log('Recognition stopped intentionally');

                // Check if restart was requested during stopping
                if (recognitionRestartPending && isCallActive) {
                    recognitionRestartPending = false;
                    setTimeout(safeStartRecognition, 100); // Reduced delay from 500ms to 100ms
                    console.log('Executing pending restart');
                }
            }

            // Update call status
            const callStatusEl = document.getElementById('callStatus');
            if (callStatusEl && isCallActive && !isProcessing) {
                callStatusEl.textContent = 'Call Active';
            }
        };
    }

    function scheduleRecognitionRestart() {
        if (recognitionState === 'stopping') {
            // Mark for restart when current stop completes
            console.log('Recognition is stopping, will restart when stopped');
            recognitionRestartPending = true;
            return;
        }

        // Stop current recognition if active
        if (recognitionState === 'active' || recognitionState === 'starting') {
            safeStopRecognition();
            setTimeout(safeStartRecognition, 100); // Reduced from 500ms
        } else {
            // Safe to start directly
            safeStartRecognition(); // Immediate start with no delay
        }
    }

    function startCall() {
        isCallActive = true;
        callStartTime = new Date();
        callSessionId = Date.now().toString();
        lastProcessedText = '';
        updateCallTimer();

        const callStatusEl = document.getElementById('callStatus');
        if (callStatusEl) {
            callStatusEl.textContent = 'Call Active - Connecting...';
            callStatusEl.classList.remove('text-danger', 'text-muted');
            callStatusEl.classList.add('text-success');
            callStatusEl.classList.remove('d-none'); // Ensure it's visible
        }

        document.getElementById('micButton').disabled = false;
        document.getElementById('micIcon').className = 'fa fa-microphone-slash text-danger';
        document.getElementById('downloadTranscript').disabled = true;

        // Initialize and start speech recognition
        initializeSpeechRecognition();
        safeStartRecognition();

        console.log('Sending start call request with session ID:', callSessionId);

        fetch('/talk-to-ai/process/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCookie('csrftoken')
            },
            body: JSON.stringify({
                text: '',
                session_id: callSessionId,
                is_call_start: true,
                is_call_end: false
            })
        })
            .then(response => {
                if (!response.ok) {
                    throw new Error(`Server responded with ${response.status}: ${response.statusText}`);
                }
                return response.json();
            })
            .then(data => {
                console.log('Start call response:', data);
                if (callStatusEl) {
                    callStatusEl.textContent = 'Call Active - Connected';
                }
                if (data.response) {
                    updateCurrentResponse(data.response);
                    // Also pass needsProcessingReset=true for initial greeting
                    speakResponse(data.response, true);
                }
            })
            .catch(error => {
                console.error('Error starting call:', error);
                updateCurrentResponse("Sorry, I couldn't connect to the server. Please try again.");
                if (callStatusEl) {
                    callStatusEl.textContent = 'Connection Error';
                    callStatusEl.classList.remove('text-success');
                    callStatusEl.classList.add('text-danger');
                }
            });
    }

    function endCall() {
        isCallActive = false;
        clearInterval(callTimer);

        // Stop speech recognition
        safeStopRecognition();
        // Also cancel any speech synthesis in progress
        window.speechSynthesis.cancel();

        // Reset flags
        isSpeaking = false;
        isProcessing = false;

        endCallUI();
        document.getElementById('downloadTranscript').disabled = false;

        console.log('Sending end call request with session ID:', callSessionId);

        fetch('/talk-to-ai/process/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCookie('csrftoken')
            },
            body: JSON.stringify({
                text: '',
                session_id: callSessionId,
                is_call_start: false,
                is_call_end: true
            })
        })
            .then(response => {
                if (!response.ok) {
                    throw new Error(`Server responded with ${response.status}: ${response.statusText}`);
                }
                return response.json();
            })
            .then(data => {
                console.log('End call response:', data);
                // Check if feedback is requested
                if (data.is_call_ended && data.request_feedback) {
                    showFeedbackForm();
                }
                // If we got a transcript, we can store it or display it
                if (data.transcript) {
                    console.log('Received transcript data');
                    // Store transcript data globally if needed
                    window.lastCallTranscript = data.transcript;
                }
            })
            .catch(error => {
                console.error('Error ending call:', error);
            });
    }

    function endCallUI() {
        const callStatusEl = document.getElementById('callStatus');
        if (callStatusEl) {
            callStatusEl.textContent = 'Call Ended';
            callStatusEl.classList.remove('text-success');
            callStatusEl.classList.add('text-danger');
        }

        // Remove listening indicator if present
        updateListeningIndicator(false);

        // Remove speaking animation if present
        document.getElementById('currentResponse').classList.remove('speaking');

        document.getElementById('micButton').disabled = false;
        document.getElementById('micIcon').className = 'fa fa-microphone text-primary';
    }

    function updateCallTimer() {
        callTimer = setInterval(() => {
            const now = new Date();
            const diff = now - callStartTime;
            const minutes = Math.floor(diff / 60000);
            const seconds = Math.floor((diff % 60000) / 1000);
            document.getElementById('callTimer').textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
        }, 1000);
    }

    function processVoiceInput(text) {
        if (!text.trim()) return;

        // Check if this is likely an echo of what the assistant just said
        if (isEchoOfAssistant(text)) {
            console.log('Echo detected, ignoring input:', text);

            // Reset processing and restart recognition without processing this input
            setTimeout(() => {
                if (isCallActive && !isProcessing && !isSpeaking) {
                    scheduleRecognitionRestart();
                }
            }, 100); // Reduced from 500ms

            return;
        }

        // Set processing flag to prevent multiple calls
        isProcessing = true;
        safeStopRecognition(); // Stop recognition while processing

        document.getElementById('processingIndicator').classList.remove('d-none');

        // Update the call status
        const callStatusEl = document.getElementById('callStatus');
        if (callStatusEl) {
            callStatusEl.textContent = 'Processing...';
        }

        console.log('Processing voice input:', text, 'with session ID:', callSessionId);

        // Add the user message to the conversation log
        conversationLog.push({
            speaker: 'user',
            message: text.trim(),
            timestamp: new Date().toISOString()
        });

        // Update current response to show user's message
        document.getElementById('currentResponse').innerHTML =
            `<div class="mb-2"><strong>You:</strong> ${text.trim()}</div>
         <div class="mt-2">Assistant is thinking...</div>`;

        fetch('/talk-to-ai/process/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCookie('csrftoken')
            },
            body: JSON.stringify({
                text: text,
                session_id: callSessionId,
                is_call_start: false,
                is_call_end: false
            })
        })
            .then(response => {
                if (!response.ok) {
                    throw new Error(`Server responded with ${response.status}: ${response.statusText}`);
                }
                return response.json();
            })
            .then(data => {
                console.log('Process voice response:', data);
                document.getElementById('processingIndicator').classList.add('d-none');

                // Update call status
                if (callStatusEl) {
                    callStatusEl.textContent = 'Assistant speaking...';
                }

                if (data.response) {
                    // Set a flag to reset processing when speech ends
                    const needsProcessingReset = true;
                    updateCurrentResponse(data.response);
                    speakResponse(data.response, needsProcessingReset);
                } else {
                    // No valid response received, reset processing flag
                    isProcessing = false;

                    // Update call status
                    if (callStatusEl) {
                        callStatusEl.textContent = 'Call Active';
                    }

                    if (isCallActive && !isSpeaking) {
                        scheduleRecognitionRestart();
                    }
                }
            })
            .catch(error => {
                document.getElementById('processingIndicator').classList.add('d-none');
                console.error('Error processing voice input:', error);
                updateCurrentResponse("Sorry, I couldn't process your request. Please try again.");

                // Update call status
                if (callStatusEl) {
                    callStatusEl.textContent = 'Call Active - Error';
                }

                // Reset processing flag on error
                isProcessing = false;
                if (isCallActive && !isSpeaking) {
                    scheduleRecognitionRestart();
                }
            });
    }

    function updateCurrentResponse(text) {
        const currentResponse = document.getElementById('currentResponse');
        currentResponse.innerHTML = `<span>${text}</span>`;
        conversationLog.push({
            speaker: 'assistant',
            message: text,
            timestamp: new Date().toISOString()
        });
    }

    function speakResponse(text, needsProcessingReset) {
        // Update last assistant message for echo detection
        lastAssistantMessage = text.toLowerCase().trim();

        // Mark as speaking
        isSpeaking = true;

        // Stop recognition while speaking
        safeStopRecognition();
        updateCurrentResponse(text);

        const utterance = new SpeechSynthesisUtterance(text);
        utterance.rate = 1.0;
        utterance.pitch = 1.0;

        // Remove speaking animation when done
        document.getElementById('currentResponse').classList.add('speaking');

        // When speaking ends
        utterance.onend = function () {
            console.log('Assistant finished speaking');
            isSpeaking = false;

            // Reset processing flag if this was part of a voice input process
            if (needsProcessingReset) {
                console.log('Resetting processing flag after assistant response');
                isProcessing = false;
            }

            // Remove speaking animation
            document.getElementById('currentResponse').classList.remove('speaking');

            // Start listening immediately after speech finishes
            if (isCallActive && !isProcessing) {
                console.log('Immediately restarting recognition after speech finished');
                forceRestartRecognition();
            } else {
                console.log('Not restarting recognition. Call active:', isCallActive, 'Processing:', isProcessing);
            }
        };

        // Also handle errors
        utterance.onerror = function (e) {
            console.log('Speech synthesis error:', e);
            isSpeaking = false;

            // Reset processing flag if this was part of a voice input process
            if (needsProcessingReset) {
                console.log('Resetting processing flag after speech error');
                isProcessing = false;
            }

            // Remove speaking animation
            document.getElementById('currentResponse').classList.remove('speaking');

            // Start listening immediately
            if (isCallActive && !isProcessing) {
                forceRestartRecognition();
            }
        };

        // Add visual feedback for assistant speaking
        window.speechSynthesis.speak(utterance);

        // Safety timeout - if speech doesn't complete within 15s, force reset
        setTimeout(() => {
            if (isSpeaking) {
                console.log('Speech timeout - force resetting speech state');
                isSpeaking = false;

                // Reset processing flag if this was part of a voice input process
                if (needsProcessingReset) {
                    console.log('Resetting processing flag after speech timeout');
                    isProcessing = false;
                }

                document.getElementById('currentResponse').classList.remove('speaking');
                if (isCallActive && !isProcessing) {
                    forceRestartRecognition();
                }
            }
        }, 15000);

        // Additional safety timeout to ensure recognition always restarts
        // This runs regardless of speech completion status
        setTimeout(() => {
            console.log('Safety check: Ensuring recognition is active after assistant response');

            // Force reset processing flag if it's still set after 16 seconds
            if (needsProcessingReset && isProcessing) {
                console.log('Force resetting processing flag that was stuck');
                isProcessing = false;
            }

            if (isCallActive && !isProcessing && !isSpeaking && recognitionState === 'inactive') {
                console.log('Recognition found inactive after assistant response - force restarting');
                forceRestartRecognition();
            }
        }, 16000);
    }

    // Forcibly restart recognition by reinitializing
    function forceRestartRecognition() {
        console.log('Force restarting recognition, current state:', recognitionState);
        try {
            // First completely stop and cleanup
            safeStopRecognition();

            // Wait minimal time for cleanup and restart immediately
            setTimeout(() => {
                try {
                    // Then reinitialize completely
                    initializeSpeechRecognition();

                    // And start again immediately
                    safeStartRecognition();

                    // Double-check recognition is running after 1 second
                    setTimeout(() => {
                        if (isCallActive && recognitionState !== 'active' && recognitionState !== 'starting') {
                            console.log('Recognition still not active after restart attempt, trying one more time');
                            initializeSpeechRecognition();
                            safeStartRecognition();
                        }
                    }, 1000);
                } catch (innerError) {
                    console.error('Error starting recognition in force restart:', innerError);
                    initializeSpeechRecognition();
                    safeStartRecognition();
                }
            }, 100); // Reduced from 300ms to 100ms
        } catch (e) {
            console.error('Error in force restart:', e);
            // Last resort - brute force reinitialize
            initializeSpeechRecognition();
            safeStartRecognition();
        }
    }

    // Update visual listening indicator
    function updateListeningIndicator(isListening) {
        // Remove old indicator if it exists
        if (listenIndicator) {
            listenIndicator.remove();
            listenIndicator = null;
        }

        if (isListening) {
            // Create and add listening indicator
            listenIndicator = document.createElement('div');
            listenIndicator.className = 'listening-indicator';
            listenIndicator.innerHTML = `
            <div class="listening-waves">
                <div class="wave"></div>
                <div class="wave"></div>
                <div class="wave"></div>
            </div>
            <div class="mt-1 text-primary"><small>Listening...</small></div>
        `;
            document.getElementById('micButton').appendChild(listenIndicator);
            document.getElementById('micIcon').classList.add('listening');
        } else {
            // Remove listening class
            document.getElementById('micIcon').classList.remove('listening');
        }
    }

    function isEchoOfAssistant(text) {
        // Ignore if we don't have a last message
        if (!lastAssistantMessage) return false;

        // Normalize both texts
        const normalizedInput = text.toLowerCase().trim();
        const normalizedLast = lastAssistantMessage.toLowerCase().trim();

        // Skip very short inputs
        if (normalizedInput.length < 4) return false;

        // Check if user input contains large portions of the assistant's last message
        // or if they're very similar
        const similarity = calculateStringSimilarity(normalizedInput, normalizedLast);
        console.log(`Echo check - Similarity: ${similarity}, Input: "${normalizedInput}", Last: "${normalizedLast}"`);

        // If similarity is above threshold, it's likely an echo
        return similarity > 0.7; // 70% similarity threshold
    }

    function calculateStringSimilarity(str1, str2) {
        // Simple function to calculate similarity between two strings
        // Returns value between 0 (completely different) and 1 (identical)

        // If either string is empty
        if (!str1.length || !str2.length) return 0;

        // Check if one string contains the other
        if (str1.includes(str2) || str2.includes(str1)) {
            const longerLength = Math.max(str1.length, str2.length);
            const shorterLength = Math.min(str1.length, str2.length);
            return shorterLength / longerLength;
        }

        // Count common words
        const words1 = str1.split(/\s+/);
        const words2 = str2.split(/\s+/);
        let commonWords = 0;

        for (const word of words1) {
            if (word.length < 3) continue; // Skip very short words
            if (words2.includes(word)) {
                commonWords++;
            }
        }

        // Calculate similarity based on word overlap
        const totalUniqueWords = new Set([...words1, ...words2]).size;
        return commonWords / totalUniqueWords;
    }

    function downloadTranscript() {
        const transcript = conversationLog.map(log => {
            const time = new Date(log.timestamp).toLocaleTimeString();
            return `[${time}] ${log.speaker.toUpperCase()}: ${log.message}`;
        }).join('\n\n');
        const blob = new Blob([transcript], { type: 'text/plain' });
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `call-transcript-${new Date().toISOString().split('T')[0]}.txt`;
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
        document.body.removeChild(a);
    }

    function getCookie(name) {
        let cookieValue = null;
        if (document.cookie && document.cookie !== '') {
            const cookies = document.cookie.split(';');
            for (let i = 0; i < cookies.length; i++) {
                const cookie = cookies[i].trim();
                if (cookie.substring(0, name.length + 1) === (name + '=')) {
                    cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                    break;
                }
            }
        }
        return cookieValue;
    }

    // Feedback Management Functions
    function showFeedbackForm() {
        document.getElementById('feedbackForm').classList.remove('d-none');
        resetFeedbackForm();
        initializeStarRating();
    }

    function hideFeedbackForm() {
        document.getElementById('feedbackForm').classList.add('d-none');
        resetFeedbackForm();
    }

    function resetFeedbackForm() {
        selectedRating = 0;
        feedbackSubmitted = false;
        document.getElementById('feedbackComments').value = '';
        document.getElementById('helpfulAspects').value = '';
        document.getElementById('improvementSuggestions').value = '';
        document.getElementById('submitFeedback').disabled = true;
        document.getElementById('feedbackStatus').classList.add('d-none');
        document.getElementById('ratingText').textContent = 'Please select a rating';
        
        // Reset star display
        const stars = document.querySelectorAll('.star');
        stars.forEach(star => star.classList.remove('active'));
    }

    function initializeStarRating() {
        const stars = document.querySelectorAll('.star');
        const ratingText = document.getElementById('ratingText');
        const submitButton = document.getElementById('submitFeedback');

        const ratingTexts = {
            1: 'Very Poor',
            2: 'Poor', 
            3: 'Average',
            4: 'Good',
            5: 'Excellent'
        };

        stars.forEach(star => {
            star.addEventListener('click', function() {
                const rating = parseInt(this.dataset.rating);
                selectedRating = rating;

                // Update star display
                stars.forEach((s, index) => {
                    if (index < rating) {
                        s.classList.add('active');
                    } else {
                        s.classList.remove('active');
                    }
                });

                // Update rating text
                ratingText.textContent = ratingTexts[rating];
                ratingText.classList.remove('text-muted');
                ratingText.classList.add('text-primary');

                // Enable submit button
                submitButton.disabled = false;
            });

            // Add hover effects
            star.addEventListener('mouseenter', function() {
                const rating = parseInt(this.dataset.rating);
                stars.forEach((s, index) => {
                    if (index < rating) {
                        s.style.opacity = '1';
                    } else {
                        s.style.opacity = '0.3';
                    }
                });
            });
        });

        // Reset hover on mouse leave
        document.getElementById('starRating').addEventListener('mouseleave', function() {
            stars.forEach((s, index) => {
                if (index < selectedRating) {
                    s.style.opacity = '1';
                } else {
                    s.style.opacity = '0.3';
                }
            });
        });
    }

    function submitFeedback() {
        if (selectedRating === 0) {
            alert('Please select a rating before submitting.');
            return;
        }

        const feedbackData = {
            session_id: callSessionId,
            rating: selectedRating,
            comments: document.getElementById('feedbackComments').value.trim(),
            helpful_aspects: document.getElementById('helpfulAspects').value.trim(),
            improvement_suggestions: document.getElementById('improvementSuggestions').value.trim()
        };

        // Disable submit button and show loading state
        const submitButton = document.getElementById('submitFeedback');
        const originalText = submitButton.textContent;
        submitButton.disabled = true;
        submitButton.textContent = 'Submitting...';

        fetch('/talk-to-ai/feedback/', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-CSRFToken': getCookie('csrftoken')
            },
            body: JSON.stringify(feedbackData)
        })
        .then(response => {
            if (!response.ok) {
                throw new Error(`Server responded with ${response.status}: ${response.statusText}`);
            }
            return response.json();
        })
        .then(data => {
            console.log('Feedback submitted successfully:', data);
            
            // Show success message
            document.getElementById('feedbackStatus').classList.remove('d-none');
            document.getElementById('feedbackStatus').innerHTML = 
                '<small class="text-success"><i class="fa fa-check"></i> Thank you for your feedback!</small>';
            
            feedbackSubmitted = true;

            // Hide form after 2 seconds
            setTimeout(() => {
                hideFeedbackForm();
            }, 2000);
        })
        .catch(error => {
            console.error('Error submitting feedback:', error);
            
            // Show error message
            document.getElementById('feedbackStatus').classList.remove('d-none');
            document.getElementById('feedbackStatus').innerHTML = 
                '<small class="text-danger"><i class="fa fa-exclamation-triangle"></i> Error submitting feedback. Please try again.</small>';
            
            // Re-enable submit button
            submitButton.disabled = false;
            submitButton.textContent = originalText;
        });
    }

    document.getElementById('micButton').addEventListener('click', function () {
        console.log('Microphone button clicked. Current state:', isCallActive ? 'Active' : 'Inactive');
        if (!isCallActive) {
            startCall();
        } else {
            endCall();
        }
    });
    document.getElementById('downloadTranscript').addEventListener('click', downloadTranscript);

    // Feedback form event listeners
    document.getElementById('submitFeedback').addEventListener('click', submitFeedback);
    
    document.getElementById('skipFeedback').addEventListener('click', function() {
        hideFeedbackForm();
    });

    // Test API button (only if it exists)
    const testApiButton = document.getElementById('testApiButton');
    if (testApiButton) {
        testApiButton.addEventListener('click', function () {
            fetch('/talk-to-ai/test-api/')
                .then(response => response.json())
                .then(data => {
                    alert('API Test Result: ' + JSON.stringify(data));
                    console.log('API Test Result:', data);
                })
                .catch(error => {
                    alert('API Test Failed: ' + error);
                    console.error('API Test Error:', error);
                });
        });
    }

    // Add periodic check for stuck states
    setInterval(() => {
        if (isCallActive) {
            // If processing has been true for more than 20 seconds, it's probably stuck
            if (isProcessing && processingStuckSince === null) {
                processingStuckSince = Date.now();
            } else if (isProcessing && processingStuckSince !== null) {
                const stuckTime = Date.now() - processingStuckSince;
                if (stuckTime > 20000) { // 20 seconds
                    console.log('Processing state appears stuck for', stuckTime / 1000, 'seconds - resetting');
                    isProcessing = false;
                    processingStuckSince = null;

                    // Also unstick recognition if needed
                    if (recognitionState !== 'active' && !isSpeaking) {
                        console.log('Attempting to restart recognition after unsticking processing state');
                        forceRestartRecognition();
                    }
                }
            } else {
                processingStuckSince = null;
            }
        }
    }, 5000);
</script>
{% endblock %}